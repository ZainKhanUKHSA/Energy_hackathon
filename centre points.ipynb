{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bcbc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load GeoJSON file\n",
    "with open(\"data/brownfield-land.geojson\", 'r', encoding='utf-8') as f:\n",
    "    geojson_data = json.load(f)\n",
    "\n",
    "# Access features or properties\n",
    "print(geojson_data['features'][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6504e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bad760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read GeoJSON file into GeoPandas DataFrame\n",
    "file_path = r\"data/brownfield-land.geojson\"\n",
    "geo_df = gpd.read_file(file_path)\n",
    "\n",
    "# Plot the map\n",
    "geo_df.plot(figsize=(10, 10), color='blue', edgecolor='black')\n",
    "plt.title(\"Brownfield Land Map\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5534ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load GeoJSON file\n",
    "with open(\"data/Lower_layer_Super_Output_Areas_December_2021_Boundaries_EW_BSC_V4_-4299016806856585929.geojson\", 'r', encoding='utf-8') as f:\n",
    "    geojson_lsoa = json.load(f)\n",
    "\n",
    "# Access features or properties\n",
    "print(geojson_lsoa['features'][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_lsoa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f36c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "\n",
    "# Assuming geojson_lsoa is the loaded GeoJSON dictionary\n",
    "features = geojson_lsoa['features']\n",
    "\n",
    "# Extract geometries\n",
    "geometries = [shape(feature['geometry']) for feature in features]\n",
    "\n",
    "# Create a GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(features, geometry=geometries)\n",
    "\n",
    "# Calculate centroids\n",
    "gdf['centroid'] = gdf.geometry.centroid\n",
    "\n",
    "# Print the centroids along with the ID\n",
    "print(gdf[['properties', 'centroid']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5de627",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gdf['properties'] = gdf['properties'].apply(lambda x: x['LSOA21CD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f23736",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.rename(columns={'properties': 'LSOA code'})\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fefa756",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_excel(\"data/gdf_export.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f5bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming geojson_lsoa is the loaded GeoJSON dictionary\n",
    "features = geojson_lsoa['features']\n",
    "\n",
    "# Extract geometries\n",
    "geometries = [shape(feature['geometry']) for feature in features]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(features)\n",
    "\n",
    "# Extract geometries and calculate centroids\n",
    "df['geometry'] = geometries\n",
    "df['centroid'] = df['geometry'].apply(lambda geom: geom.centroid)\n",
    "\n",
    "# Print the centroids along with the ID\n",
    "print(df[['properties', 'centroid']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5fad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788fa52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = gpd.read_file(file_path)\n",
    "\n",
    "# Extract properties into a DataFrame\n",
    "properties_df = pd.DataFrame(geo_df.drop(columns='geometry'))\n",
    "\n",
    "# Calculate centroids\n",
    "geo_df['centroid'] = geo_df.geometry.centroid\n",
    "\n",
    "# Combine properties and centroids into a new DataFrame\n",
    "combined_df = properties_df.copy()\n",
    "combined_df['centroid'] = geo_df['centroid']\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(combined_df.head())\n",
    "\n",
    "# Plot the map\n",
    "geo_df.plot(figsize=(10, 10), color='blue', edgecolor='black')\n",
    "plt.title(\"Brownfield Land Map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather= pd.DataFrame(gdf['LSOA code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f2cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7555ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "weather['avg_precip_mm'] = weather.apply(lambda x: np.random.uniform(100, 2274), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f4b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['cloud_okta'] = weather['avg_precip_mm'].apply(\n",
    "    lambda x: np.random.randint(5, 9) if x > 1000 else \n",
    "              np.random.randint(2, 5) if 200 < x <= 1000 else \n",
    "              np.random.randint(0, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8935905",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['wind_mph'] = weather['avg_precip_mm'].apply(\n",
    "    lambda x: round(np.random.uniform(20, 63), 1) if x > 1800 else \n",
    "              round(np.random.uniform(0, 50), 1) if 800 < x <= 1800 else \n",
    "              round(np.random.uniform(0, 25), 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4081cd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13456136",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.to_excel(\"data/dummy_weather.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b4d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfl=pd.read_csv(\"data/brownfield-land.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab57319",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf4d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping NAs from the dataset before converting to string, important has it will convert NAs to nan strings\n",
    "bfl =bfl.dropna(subset=['notes'])\n",
    "\n",
    "# counts for the history text column\n",
    "bfl_t= bfl['notes'].str.split(expand=True).unstack().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cef34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d0f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4f0475",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e087bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guidance for text preprocessing was used from Shanawad V. (2022) Elghali R. (2021), Anistropic (2018)\n",
    "# nltk downloads\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "#nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2290010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general libaries for basic data cleaning\n",
    "import plotly.offline as py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "# nltyk packages and spacy. Helps with pre-processing text\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data visualisation\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import pickle \n",
    "\n",
    "from pprint import pprint\n",
    "from IPython.core.display import display, HTML\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88742daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a34c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk downloads\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204bdf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet, stopwords\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707b0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfde3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c56794",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9cfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# controlling how the words get converted\n",
    "def tagger(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c30b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def drop_stopwords(text):\n",
    "    # tokenises\n",
    "    tokens = text\n",
    "    dropped = [word for word in tokens if word.lower() not in stopwords]\n",
    "    final_text = ' '.join(dropped)\n",
    "    return final_text\n",
    "\n",
    "# Shanawad V. (2022) Elghali R. (2021)\n",
    "def text_clean(text):\n",
    "    # convert text to string\n",
    "    text = str(text)\n",
    "    # identified some problematic names which could negatively impact. So combined\n",
    "\n",
    "    text = re.sub(r'(Sector)\\s+(\\d+)', r'\\1\\2', text)\n",
    "\n",
    "    # removing, non words, punction and web links and new lines\n",
    "\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\[.*?\\]', ' ', text)\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text) \n",
    "    text = re.sub(r'<.*?>+', ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    # removing digits not attached to names\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    \n",
    "    return text\n",
    "def lemma_tag(sentence):\n",
    "    sentence_words = word_tokenize(sentence)\n",
    "    pos_tags = nltk.pos_tag(sentence_words)\n",
    "    lemmatized_words = [wnl.lemmatize(word, pos=tagger(tag)) for word, tag in pos_tags]\n",
    "    lemmatized_words =[word.lower() for word in lemmatized_words]\n",
    "    # Join the lemmatized words into a single string\n",
    "    lemmatized_sentence = ' '.join(lemmatized_words)\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90854671",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfl.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e237bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping NAs from the dataset before converting to string, important has it will convert NAs to nan strings\n",
    "bfl =bfl.dropna(subset=['notes'])\n",
    "\n",
    "# counts for the history text column\n",
    "bf11= bfl['notes'].str.split(expand=True).unstack().value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d322fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk downloads\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "#stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac9bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d158860",
   "metadata": {},
   "outputs": [],
   "source": [
    "example=bfl['notes'][0]\n",
    "\n",
    "print(\"Original:\",example)\n",
    "clean= text_clean(example)\n",
    "print(\"\\ncleaning:\",clean)\n",
    "lem= lemma_tag(clean)\n",
    "print(\"\\nLemmatisation:\",lem)\n",
    "stpword= drop_stopwords(lem)\n",
    "print(\"\\nStop word removal:\",stpword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e8a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0deccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "bfl['lemma']=bfl['notes'].apply(text_clean).apply(lemma_tag).apply(drop_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6493ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lda_lemma = bfl['lemma'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703bf0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud\n",
    "lda_text = ' '.join(bfl['lemma'])\n",
    "\n",
    "# create word cload\n",
    "wordcloud = WordCloud(width=1000, height=400).generate(lda_text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf080e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud\n",
    "lda_text = ' '.join(bfl_t['notes'])\n",
    "\n",
    "# create word cload\n",
    "wordcloud = WordCloud(width=1000, height=400).generate(lda_text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
